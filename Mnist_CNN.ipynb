{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mnist_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0JfQTgR6Z2f"
      },
      "source": [
        "%load_ext tensorboard\r\n",
        "import tensorflow as tf\r\n",
        "import datetime, os\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "\r\n",
        "from pathlib import Path\r\n",
        "import requests\r\n",
        "import gzip\r\n",
        "import numpy as np\r\n",
        "from torch.utils.data import TensorDataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "from torch import nn\r\n",
        "from torch import optim\r\n",
        "from torch.nn import functional as F\r\n",
        "import torch\r\n",
        "\r\n",
        "def get_data_set(mini_batch = 32, validation = False):\r\n",
        "    image_size = 28\r\n",
        "    num_images_train = 60000\r\n",
        "    num_images_test = 10000\r\n",
        "\r\n",
        "    PATH = Path(\".\", \"data\", \"mnist\")\r\n",
        "    if not PATH.is_dir():\r\n",
        "        PATH.mkdir(parents = True, exist_ok = True)\r\n",
        "    \r\n",
        "    URL = \"http://yann.lecun.com/exdb/mnist/\"\r\n",
        "    train_x_file = \"train-images-idx3-ubyte.gz\"\r\n",
        "    train_y_file = \"train-labels-idx1-ubyte.gz\"\r\n",
        "    test_x_file = \"t10k-images-idx3-ubyte.gz\"\r\n",
        "    test_y_file = \"t10k-labels-idx1-ubyte.gz\"\r\n",
        "    \r\n",
        "    for f in [train_x_file,train_y_file,test_x_file,test_y_file]:\r\n",
        "        \"\"\"if not (PATH / f).exists():\r\n",
        "            content = requests.get(URL + f).content\r\n",
        "            (PATH / f).open(\"wb\").write(content)\"\"\"\r\n",
        "        \r\n",
        "        with gzip.open((PATH / f), \"rb\") as d:\r\n",
        "            if f == train_x_file:\r\n",
        "                d.read(16)\r\n",
        "                buf = d.read(image_size * image_size * num_images_train)\r\n",
        "                data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\r\n",
        "                train_x = data.reshape(num_images_train, image_size*image_size)\r\n",
        "            elif f == train_y_file:\r\n",
        "                d.read(8)\r\n",
        "                buf = d.read(num_images_train)\r\n",
        "                train_y = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\r\n",
        "                #train_y = data.reshape(data.shape[0],1)\r\n",
        "                \r\n",
        "                \"\"\"first = True\r\n",
        "                for i in data:\r\n",
        "                    if first == True:\r\n",
        "                        train_y = torch.zeros(1,10, dtype=torch.int64)\r\n",
        "                        train_y[0,i] = 1\r\n",
        "                        first = False\r\n",
        "                    else:\r\n",
        "                        y = torch.zeros(1,10, dtype=torch.int64)\r\n",
        "                        y[0,i] = 1\r\n",
        "                        train_y = torch.cat((train_y,y), 0)\r\n",
        "                first = True\r\n",
        "                for i in data:\r\n",
        "                    if first == True:\r\n",
        "                        train_y = torch.from_numpy(data)\r\n",
        "                        first = False\r\n",
        "                    else:\r\n",
        "                        train_y = torch.cat((train_y,torch.from_numpy(data)), 0)\"\"\"\r\n",
        "                \r\n",
        "            elif f == test_x_file:\r\n",
        "                d.read(16)\r\n",
        "                buf = d.read(image_size * image_size * num_images_test)\r\n",
        "                data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\r\n",
        "                test_x = data.reshape(num_images_test, image_size*image_size)\r\n",
        "            elif f == test_y_file:\r\n",
        "                d.read(8)\r\n",
        "                buf = d.read(num_images_test)\r\n",
        "                test_y = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\r\n",
        "                #test_y = data.reshape(data.shape[0],1)\r\n",
        "\r\n",
        "                \"\"\"first = True\r\n",
        "                for i in data:\r\n",
        "                    if first == True:\r\n",
        "                        test_y = torch.zeros(1,10, dtype=torch.int64)\r\n",
        "                        test_y[0,i] = 1\r\n",
        "                        first = False\r\n",
        "                    else:\r\n",
        "                        y = torch.zeros(1,10, dtype=torch.int64)\r\n",
        "                        y[0,i] = 1\r\n",
        "                        test_y = torch.cat((test_y,y), 0)\r\n",
        "                first = True\r\n",
        "                for i in data:\r\n",
        "                    if first == True:\r\n",
        "                        test_y = torch.from_numpy(data)\r\n",
        "                        first = False\r\n",
        "                    else:\r\n",
        "                        test_y = torch.cat((test_y,torch.from_numpy(data)), 0)\"\"\"\r\n",
        "            \r\n",
        "    (train_x,train_y,test_x,test_y) = map(torch.tensor, (train_x,train_y,test_x,test_y))\r\n",
        "    train_y.type(torch.int64)\r\n",
        "    test_y.type(torch.int64)\r\n",
        "\r\n",
        "    if validation:\r\n",
        "        data_set_train = TensorDataset(train_x[:50000, : ],train_y[:50000])\r\n",
        "        data_set_validation = TensorDataset(train_x[50000:, : ],train_y[50000:])\r\n",
        "        data_set_test = TensorDataset(test_x, test_y)\r\n",
        "        \r\n",
        "        return (DataLoader(data_set_train, mini_batch, shuffle = True),\r\n",
        "                DataLoader(data_set_train, len(data_set_train)),\r\n",
        "                DataLoader(data_set_validation, len(data_set_validation)),\r\n",
        "                DataLoader(data_set_test, len(data_set_test)))\r\n",
        "    else:\r\n",
        "        data_set_train = TensorDataset(train_x,train_y)\r\n",
        "        data_set_test = TensorDataset(test_x, test_y)\r\n",
        "    \r\n",
        "        return (DataLoader(data_set_train, mini_batch, shuffle = True),\r\n",
        "                DataLoader(data_set_train, len(data_set_train)),\r\n",
        "                None,\r\n",
        "                DataLoader(data_set_test, len(data_set_test)))\r\n",
        " \r\n",
        "\r\n",
        "class Function_class(nn.Module):\r\n",
        "    def __init__(self, function):\r\n",
        "        super().__init__()\r\n",
        "        self.function = function\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        return self.function(x)\r\n",
        "\r\n",
        "def Convert_array_to_image(x):\r\n",
        "    return x.view(-1,1,28,28)\r\n",
        "\r\n",
        "def Convert_image_to_array(x):\r\n",
        "    size = x.size()[1:] \r\n",
        "    num_features = 1\r\n",
        "    for s in size:\r\n",
        "        num_features *= s\r\n",
        "    return x.view(-1, num_features)\r\n",
        "\r\n",
        "class Mnist_CNN(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.layers = nn.Sequential(\r\n",
        "            Function_class(Convert_array_to_image),\r\n",
        "            nn.Conv2d(1, 20, 5),\r\n",
        "            nn.MaxPool2d(2,1),\r\n",
        "            Function_class(Convert_image_to_array),\r\n",
        "            nn.Linear(10580, 1000),\r\n",
        "            nn.Sigmoid(),\r\n",
        "            nn.Linear(1000, 100),\r\n",
        "            nn.Sigmoid(),\r\n",
        "            nn.Linear(100,10))\r\n",
        "    \r\n",
        "    def forward(self,x):\r\n",
        "        return self.layers(x)\r\n",
        "\r\n",
        "def get_model(lr = 1e-3):\r\n",
        "    model = Mnist_CNN()\r\n",
        "    model = model.to(dev)\r\n",
        "    optimizer = optim.SGD(model.parameters(), lr = lr)\r\n",
        "    return model, optimizer\r\n",
        "\r\n",
        "def fit(data_set_train, data_set_train_complete, data_set_validation, data_set_test, epoch = 60):\r\n",
        "    model, optimizer = get_model()\r\n",
        "    for e in range(epoch):\r\n",
        "        for x,y in data_set_train:\r\n",
        "            loss_function = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "            loss = loss_function(model(x.to(dev)), y.to(dev))\r\n",
        "            \r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            optimizer.zero_grad()\r\n",
        "        \r\n",
        "        Evaluate_loss(model, loss_function, data_set_train_complete, \"Loss/train\", e)\r\n",
        "        Evaluate_loss(model, loss_function, data_set_validation,\"Loss/validation\", e)\r\n",
        "        \r\n",
        "        Evaluate_accuracy(model, loss_function, data_set_train_complete, \"Accuracy/train\", e)\r\n",
        "        Evaluate_accuracy(model, loss_function, data_set_validation, \"Accuracy/validation\", e)\r\n",
        "        \r\n",
        "    Evaluate_loss(model, loss_function, data_set_test, \"Loss/test\", e)\r\n",
        "    Evaluate_accuracy(model, loss_function, data_set_test, \"Accuracy/test\", e)\r\n",
        "\r\n",
        "def Evaluate_loss(model, loss_function, data_set, kind_data_set, epoch):\r\n",
        "    for x,y in data_set:\r\n",
        "        loss = loss_function(model(x.to(dev)), y.to(dev))\r\n",
        "        graph.add_scalar(kind_data_set, loss, epoch)\r\n",
        "        print(kind_data_set + \": \" + str(loss), end = \"\\n\")\r\n",
        "\r\n",
        "def Evaluate_accuracy(model, loss_function, data_set, kind_data_set, epoch):\r\n",
        "    for x,y in data_set:\r\n",
        "        pred = model(x.to(dev))\r\n",
        "        pred = torch.argmax(pred, dim = 1)\r\n",
        "        \r\n",
        "        accuracy = (pred == y.to(dev)).float().mean()\r\n",
        "        graph.add_scalar(kind_data_set, accuracy, epoch)\r\n",
        "\r\n",
        "        print(kind_data_set + \": \" + str(accuracy), end = \"\\n\")\r\n",
        "                \r\n",
        "graph = SummaryWriter()\r\n",
        "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\r\n",
        "print(dev)\r\n",
        "data_set_train, data_set_train_complete, data_set_validation, data_set_test = get_data_set(validation = True)\r\n",
        "fit(data_set_train, data_set_train_complete, data_set_validation, data_set_test)\r\n",
        "graph.close()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy6ZoO9dWWMO"
      },
      "source": [
        "%tensorboard --logdir=./runs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}